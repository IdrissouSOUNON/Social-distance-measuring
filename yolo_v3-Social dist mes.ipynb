{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO V3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow\n",
    "# !mkdir data\n",
    "# !wget wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights\n",
    "# !wget https://pjreddie.com/media/files/yolov3-tiny.weights -O data/yolov3-tiny.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, datetime, imutils, math\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.spatial import distance\n",
    "from centroidtracker import CentroidTracker\n",
    "\n",
    "CONFIDENCE_THRESH = 0.5 # Seuil minimal de detection d'une personne\n",
    "nms_thresh = 0.5 # Seuil minimal de detection\n",
    "MIN_DISTANCE = 100 # ?\n",
    "VIDEO_RECORD_SPEED = 18.0\n",
    "\n",
    "classes_file = \"coco.names\" # fichier texte contenant chaque classe d'objet détecté\n",
    "weights = \"yolov3.weights\" # poids des paramètres du model pré-entraîné\n",
    "config_file = \"yolov3.cfg\" # Ensemble des paramètres du model pré-entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de yolo v3 en utilisant cv2.dnn.readNet en passant les poids et les paramètres\n",
    "net = cv2.dnn.readNet(weights, config_file)\n",
    "\n",
    "# Lecture des noms des classes à partir du fichier texte \"coco.names\"\n",
    "classes = []\n",
    "with open(classes_file, \"r\") as f:\n",
    "    classes = f.read().splitlines()\n",
    "    \n",
    "# exécution des inférences sur le réseau et collection des prédictions à partir des couches de sortie\n",
    "# output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "# layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "\n",
    "# défintion des couches de sortie car c'est là que nous définirons quel objet est détecté\n",
    "layer_names = net.getLayerNames()\n",
    "layerOutputs = [layer_names [i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors= np.random.uniform(0,255,size=(len(classes),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuite, chargeons une image. Nous réduirons la hauteur et la largeur de notre image à une échelle \n",
    "# de 40% et 30%. Et enregistrez toutes ces valeurs dans les variables de hauteur, largeur, canaux pour \n",
    "# l'image d'origine.\n",
    "\n",
    "frame = cv2.imread(\"ab.jpg\")\n",
    "frame = cv2.resize(frame, None, fx=0.8, fy=0.6)\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "\n",
    "# cv2.imshow(\"Predited_Image\", frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "    \n",
    "# nous définirons les couches de sortie car c'est là que nous définirons quel objet est \n",
    "# détecté en utilisant net.getUnconnectedOutLayers et net.getLayerNames\n",
    "\n",
    "# nous ne pouvons pas donner cette image directement à l'algorithme, nous devons donc effectuer \n",
    "# une conversion à partir de cette image appelée conversion blob qui consiste \n",
    "# essentiellement à extraire des fonctionnalités de l'image\n",
    "\n",
    "# Détection des objets dans le blob en utilisant cv2.dnn.blobFromImage et en passant \n",
    "# quelques arguments: frame: la capture, 1/255.0: facteur d'échelle, taille de l'image \n",
    "# à utiliser dans le blob être (416,416), pas de soustraction moyenne des calques comme (0,0,0), \n",
    "# swapRB=True: signifie que nous inverserons le bleu avec le rouge car OpenCV utilise BGR \n",
    "# mais nous avons des canaux dans l'image en RVB.\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "\n",
    "# Voyons maintenant à quoi ressemblent les 3 objets blob différents en utilisant le code suivant. \n",
    "# Nous n'observons pas beaucoup de différence mais c'est ce que nous allons entrer dans l'algorithme YOLO.\n",
    "\n",
    "# for b in blob: \n",
    "#     for n, img_blob in enumerate(b): \n",
    "#         cv2.imshow(str(n), img_blob)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Nous transmettons maintenant cet objet blob au réseau en utilisant net.setInput (blob) , puis le \n",
    "# transmettons aux couches de sortie. Ici, tous les objets ont été détectés et outs contient toutes \n",
    "# les informations dont nous avons besoin pour extraire la position de l'objet comme les positions en haut, \n",
    "# à gauche, à droite, en bas, le nom de la classe.\n",
    "\n",
    "net.setInput(blob)\n",
    "outs = net.forward(layerOutputs)\n",
    "#     print(outs[1])\n",
    "\n",
    "\n",
    "'''\n",
    "Évaluons maintenant les sorties en affichant les informations à l'écran. Nous essaierons principalement\n",
    "de prédire la confiance, ce qui signifie à quel point l'algorithme est confiant lorsqu'il prévoit un objet. \n",
    "Pour cela, nous bouclerons les outs, obtenons d'abord tous les scores de chacun des outs . Ensuite, récupérez \n",
    "le class_id qui a le score le plus élevé parmi eux, puis attribuez une confiance à la valeur des scores en \n",
    "passant class_id .\n",
    "Nous allons maintenant attribuer le seuil du niveau de confiance à 0,5. Tout ce qui est supérieur à 0,5 \n",
    "devrait signifier un objet détecté. Soit aussi center_x, center_y, w comme largeur, h comme hauteur de \n",
    "l'objet détecté. Ici, nous allons utiliser les variables de hauteur et de largeur que nous avons enregistrées \n",
    "précédemment de l'image originale.\n",
    "En outre , dessinons un rectangle autour de l'objet détecté en utilisant center_x, center_y, w, h . \n",
    "Et ajoutez quelques informations à cela comme la classe, la confiance.\n",
    "'''\n",
    "\n",
    "# affichage des informations à l'écran / obtenir le score de confiance de l'algorithme dans la détection \n",
    "# d'un objet dans le blob\n",
    "\n",
    "\n",
    "# pour chaque détection de chaque couche de sortie, obtenez la confiance, l'ID de classe, les paramètres \n",
    "# de la boîte englobante et ignorez les détections faibles (confiance <0,5) pour chaque détection de chaque \n",
    "# couche de sortie, obtenir les paramètres de confiance, d'identification de classe, de boîte englobante et \n",
    "# ignorer les détections faibles (confiance < 0,5)\n",
    "\n",
    "\n",
    "class_ids=[]\n",
    "confidences=[]\n",
    "boxes=[]\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            #onject detected\n",
    "            center_x= int(detection[0]*width)\n",
    "            center_y= int(detection[1]*height)\n",
    "            w = int(detection[2]*width)\n",
    "            h = int(detection[3]*height)\n",
    "        \n",
    "            #cv2.circle(img,(center_x,center_y),10,(0,255,0),2)\n",
    "            \n",
    "            #rectangle co-ordinaters\n",
    "            x=int(center_x - w/2)\n",
    "            y=int(center_y - h/2)\n",
    "            #cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            \n",
    "            boxes.append([x,y,w,h]) #put all rectangle areas\n",
    "            confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "            class_ids.append(class_id) #name of the object tha was detected\n",
    "            \n",
    "\n",
    "    '''\n",
    "    \n",
    "    Il peut y avoir des cas où le même objet peut être détecté plusieurs fois comme ci-dessous. Vous voyez \n",
    "    que deux boîtes sont détectées chacune pour l'ordinateur portable et le moniteur. \n",
    "    Pour éliminer cela, nous utiliserons la fonctionnalité de suppression non maximale (NMS) qui permettar\n",
    "    d'éliminer les boîtes en utilisant une valeur de seuil de 0,6 et permet de conserver que la meilleure \n",
    "    de toutes les boîtes. Et la variable index gardera une trace de ces objets uniques détectés.Donc pas de\n",
    "    détection multiple pour le même objet.\n",
    "    \n",
    "    Ensuite, en utilisant la boucle ci-dessous sur toutes les boîtes trouvées, si la boîte apparaît dans \n",
    "    les index, l'algorithme dessinera uniquement un rectangle avec une couleur spécifique, met le texte du \n",
    "    nom de sa classe dessus.\n",
    "\n",
    "    '''\n",
    "indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FONT = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "FONT_SCALE = 1.0\n",
    "FONT_THICKNESS = 2\n",
    "TEST_COLOR = (255, 255, 255)\n",
    "label_color = (0, 0, 0)\n",
    "\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x,y,w,h = boxes[i]\n",
    "        label = '{} {:.2f}%'.format(classes[class_ids[i]],  confidences[i]*100)\n",
    "        color = colors[i]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "\n",
    "        (label_width, label_height), baseline = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n",
    "        label_patch = np.zeros((label_height + baseline, label_width, 3), np.uint8)\n",
    "        cv2.rectangle(frame, (x,y+int(0.5*baseline)),(x+label_width,y-label_height), color, FONT_THICKNESS-3)\n",
    "        cv2.putText(frame, label, (x,y), FONT, 1, TEST_COLOR, FONT_THICKNESS-1)\n",
    "        \n",
    "cv2.imshow(\"Predited_Image\", frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, datetime, imutils, math\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.spatial import distance\n",
    "from centroidtracker import CentroidTracker\n",
    "\n",
    "CONFIDENCE_THRESH = 0.5 # Seuil minimal de detection d'une personne\n",
    "nms_thresh = 0.5 # Seuil minimal de detection\n",
    "MIN_DISTANCE = 100 # ?\n",
    "VIDEO_RECORD_SPEED = 18.0\n",
    "\n",
    "classes_file = \"coco.names\" # fichier texte contenant chaque classe d'objet détecté\n",
    "weights = \"yolov3.weights\" # poids des paramètres du model pré-entraîné\n",
    "config_file = \"yolov3.cfg\" # Ensemble des paramètres du model pré-entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de yolo v3 en utilisant cv2.dnn.readNet en passant les poids et les paramètres\n",
    "net = cv2.dnn.readNet(weights, config_file)\n",
    "\n",
    "# Lecture des noms des classes à partir du fichier texte \"coco.names\"\n",
    "classes = []\n",
    "with open(classes_file, \"r\") as f:\n",
    "    classes = f.read().splitlines()\n",
    "    \n",
    "# exécution des inférences sur le réseau et collection des prédictions à partir des couches de sortie\n",
    "# output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "# layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "\n",
    "# défintion des couches de sortie car c'est là que nous définirons quel objet est détecté\n",
    "layer_names = net.getLayerNames()\n",
    "layerOutputs = [layer_names [i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors= np.random.uniform(0,255,size=(len(classes),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('market.mp4')\n",
    "\n",
    "# Configuration de la sortie de VideoWriter\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) # VIDEO_RECORD_SPEED\n",
    "codec = cv2.VideoWriter_fourcc(*\"XVID\") # MJPG\n",
    "video_outp = cv2.VideoWriter(\"Sortie.avi\", codec, fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    dt = \"AIMS-SENEGAL, \" + \"{}\".format(datetime.datetime.now())\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Ensuite, chargeons une image. Nous réduirons la hauteur et la largeur de notre image à une échelle \n",
    "    # de 40% et 30%. Et enregistrez toutes ces valeurs dans les variables de hauteur, largeur, canaux pour \n",
    "    # l'image d'origine.\n",
    "\n",
    "    frame = cv2.resize(frame, None, fx=0.8, fy=0.6)\n",
    "    height, width, channels = frame.shape\n",
    "#     height, width = frame.shape[:2] \n",
    "    \n",
    "   \n",
    "    \n",
    "    # nous ne pouvons pas donner cette image directement à l'algorithme, nous devons donc effectuer \n",
    "    # une conversion à partir de cette image appelée conversion blob qui consiste \n",
    "    # essentiellement à extraire des fonctionnalités de l'image\n",
    "    \n",
    "    # Détection des objets dans le blob en utilisant cv2.dnn.blobFromImage et en passant \n",
    "    # quelques arguments: frame: la capture, 1/255.0: facteur d'échelle, taille de l'image \n",
    "    # à utiliser dans le blob être (416,416), pas de soustraction moyenne des calques comme (0,0,0), \n",
    "    # swapRB=True: signifie que nous inverserons le bleu avec le rouge car OpenCV utilise BGR \n",
    "    # mais nous avons des canaux dans l'image en RVB.\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "\n",
    "    # Voyons maintenant à quoi ressemblent les 3 objets blob différents en utilisant le code suivant. \n",
    "    # Nous n'observons pas beaucoup de différence mais c'est ce que nous allons entrer dans l'algorithme YOLO.\n",
    "\n",
    "    # for b in blob: \n",
    "    #     for n, img_blob in enumerate(b): \n",
    "    #         cv2.imshow(str(n), img_blob)\n",
    "    #     cv2.waitKey(0)\n",
    "    #     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    # Nous transmettons maintenant cet objet blob au réseau en utilisant net.setInput (blob) , puis le \n",
    "    # transmettons aux couches de sortie. Ici, tous les objets ont été détectés et outs contient toutes \n",
    "    # les informations dont nous avons besoin pour extraire la position de l'objet comme les positions en haut, \n",
    "    # à gauche, à droite, en bas, le nom de la classe.\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(layerOutputs)\n",
    "    #     print(outs[1])\n",
    "\n",
    "\n",
    "    '''\n",
    "    Évaluons maintenant les sorties en affichant les informations à l'écran. Nous essaierons principalement\n",
    "    de prédire la confiance, ce qui signifie à quel point l'algorithme est confiant lorsqu'il prévoit un objet. \n",
    "    Pour cela, nous bouclerons les outs, obtenons d'abord tous les scores de chacun des outs . Ensuite, récupérez \n",
    "    le class_id qui a le score le plus élevé parmi eux, puis attribuez une confiance à la valeur des scores en \n",
    "    passant class_id .\n",
    "    \n",
    "    Nous allons maintenant attribuer le seuil du niveau de confiance à 0,5. Tout ce qui est supérieur à 0,5 \n",
    "    devrait signifier un objet détecté. Soit aussi center_x, center_y, w comme largeur, h comme hauteur de \n",
    "    l'objet détecté. Ici, nous allons utiliser les variables de hauteur et de largeur que nous avons enregistrées \n",
    "    précédemment de l'image originale.\n",
    "    \n",
    "    En outre , dessinons un rectangle autour de l'objet détecté en utilisant center_x, center_y, w, h . \n",
    "    Et ajoutez quelques informations à cela comme la classe, la confiance.\n",
    "    '''\n",
    "\n",
    "    # affichage des informations à l'écran / obtenir le score de confiance de l'algorithme dans la détection \n",
    "    # d'un objet dans le blob\n",
    "    \n",
    "    class_ids=[]\n",
    "    confidences=[]\n",
    "    boxes=[]\n",
    "\n",
    "    # pour chaque détection de chaque couche de sortie, obtenez la confiance, l'ID de classe, les paramètres \n",
    "    # de la boîte englobante et ignorez les détections faibles (confiance <0,5) pour chaque détection de chaque \n",
    "    # couche de sortie, obtenir les paramètres de confiance, d'identification de classe, de boîte englobante et \n",
    "    # ignorer les détections faibles (confiance < 0,5)\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                #onject detected\n",
    "                center_x= int(detection[0]*width)\n",
    "                center_y= int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                #rectangle co-ordinaters\n",
    "                x=int(center_x - w/2)\n",
    "                y=int(center_y - h/2)\n",
    "                #cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "                boxes.append([x,y,w,h]) # put all rectangle areas\n",
    "                confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "                class_ids.append(class_id) # name of the object tha was detected\n",
    "\n",
    "\n",
    "        '''\n",
    "\n",
    "        Il peut y avoir des cas où le même objet peut être détecté plusieurs fois comme ci-dessous. Vous voyez \n",
    "        que deux boîtes sont détectées chacune pour l'ordinateur portable et le moniteur. \n",
    "        Pour éliminer cela, nous utiliserons la fonctionnalité de suppression non maximale (NMS) qui permettar\n",
    "        d'éliminer les boîtes en utilisant une valeur de seuil de 0,6 et permet de conserver que la meilleure \n",
    "        de toutes les boîtes. Et la variable index gardera une trace de ces objets uniques détectés.Donc pas de\n",
    "        détection multiple pour le même objet.\n",
    "\n",
    "        Ensuite, en utilisant la boucle ci-dessous sur toutes les boîtes trouvées, si la boîte apparaît dans \n",
    "        les index, l'algorithme dessinera uniquement un rectangle avec une couleur spécifique, met le texte du \n",
    "        nom de sa classe dessus.\n",
    "\n",
    "        '''\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "#                 indexes = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESH, nms_thresh)\n",
    "\n",
    "    FONT = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "    FONT_SCALE = 1.0\n",
    "    FONT_THICKNESS = 2\n",
    "    TEST_COLOR = (255, 255, 255)\n",
    "    label_color = (0, 0, 0)\n",
    "    \n",
    "    for i in indexes.flatten():\n",
    "        x,y,w,h = boxes[i]\n",
    "        label = '{} {:.2f}%'.format(classes[class_ids[i]],  confidences[i]*100)\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "\n",
    "        (label_width, label_height), baseline = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n",
    "        label_patch = np.zeros((label_height + baseline, label_width, 3), np.uint8)\n",
    "        cv2.rectangle(frame, (x,y+int(0.5*baseline)),(x+label_width,y-label_height), color, FONT_THICKNESS-3)\n",
    "        cv2.putText(frame, label, (x,y), FONT, 1, TEST_COLOR, FONT_THICKNESS-1)\n",
    "\n",
    "        \n",
    "    cv2.putText(frame, dt, (700, 710), font, 1, (210, 195, 155), 2, cv2.LINE_8)\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        break\n",
    "    video_outp.write(frame)\n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "video_outp.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('test.mp4')\n",
    "\n",
    "# Configuration de la sortie de VideoWriter\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) # VIDEO_RECORD_SPEED\n",
    "codec = cv2.VideoWriter_fourcc(*\"XVID\") # MJPG\n",
    "video_outp = cv2.VideoWriter(\"Sortie.avi\", codec, fps, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    dt = \"AIMS-SENEGAL, \" + \"{}\".format(datetime.datetime.now())\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Ensuite, chargeons une image. Nous réduirons la hauteur et la largeur de notre image à une échelle \n",
    "    # de 40% et 30%. Et enregistrez toutes ces valeurs dans les variables de hauteur, largeur, canaux pour \n",
    "    # l'image d'origine.\n",
    "\n",
    "    frame = cv2.resize(frame, None, fx=0.6, fy=0.6)\n",
    "    height, width, channels = frame.shape\n",
    "#     height, width = frame.shape[:2] \n",
    "    \n",
    "   \n",
    "    \n",
    "    # nous ne pouvons pas donner cette image directement à l'algorithme, nous devons donc effectuer \n",
    "    # une conversion à partir de cette image appelée conversion blob qui consiste \n",
    "    # essentiellement à extraire des fonctionnalités de l'image\n",
    "    \n",
    "    # Détection des objets dans le blob en utilisant cv2.dnn.blobFromImage et en passant \n",
    "    # quelques arguments: frame: la capture, 1/255.0: facteur d'échelle, taille de l'image \n",
    "    # à utiliser dans le blob être (416,416), pas de soustraction moyenne des calques comme (0,0,0), \n",
    "    # swapRB=True: signifie que nous inverserons le bleu avec le rouge car OpenCV utilise BGR \n",
    "    # mais nous avons des canaux dans l'image en RVB.\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "\n",
    "    # Voyons maintenant à quoi ressemblent les 3 objets blob différents en utilisant le code suivant. \n",
    "    # Nous n'observons pas beaucoup de différence mais c'est ce que nous allons entrer dans l'algorithme YOLO.\n",
    "\n",
    "    # for b in blob: \n",
    "    #     for n, img_blob in enumerate(b): \n",
    "    #         cv2.imshow(str(n), img_blob)\n",
    "    #     cv2.waitKey(0)\n",
    "    #     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    # Nous transmettons maintenant cet objet blob au réseau en utilisant net.setInput (blob) , puis le \n",
    "    # transmettons aux couches de sortie. Ici, tous les objets ont été détectés et outs contient toutes \n",
    "    # les informations dont nous avons besoin pour extraire la position de l'objet comme les positions en haut, \n",
    "    # à gauche, à droite, en bas, le nom de la classe.\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(layerOutputs)\n",
    "    #     print(outs[1])\n",
    "\n",
    "\n",
    "    '''\n",
    "    Évaluons maintenant les sorties en affichant les informations à l'écran. Nous essaierons principalement\n",
    "    de prédire la confiance, ce qui signifie à quel point l'algorithme est confiant lorsqu'il prévoit un objet. \n",
    "    Pour cela, nous bouclerons les outs, obtenons d'abord tous les scores de chacun des outs . Ensuite, récupérez \n",
    "    le class_id qui a le score le plus élevé parmi eux, puis attribuez une confiance à la valeur des scores en \n",
    "    passant class_id .\n",
    "    \n",
    "    Nous allons maintenant attribuer le seuil du niveau de confiance à 0,5. Tout ce qui est supérieur à 0,5 \n",
    "    devrait signifier un objet détecté. Soit aussi center_x, center_y, w comme largeur, h comme hauteur de \n",
    "    l'objet détecté. Ici, nous allons utiliser les variables de hauteur et de largeur que nous avons enregistrées \n",
    "    précédemment de l'image originale.\n",
    "    \n",
    "    En outre , dessinons un rectangle autour de l'objet détecté en utilisant center_x, center_y, w, h . \n",
    "    Et ajoutez quelques informations à cela comme la classe, la confiance.\n",
    "    '''\n",
    "\n",
    "    # affichage des informations à l'écran / obtenir le score de confiance de l'algorithme dans la détection \n",
    "    # d'un objet dans le blob\n",
    "    \n",
    "#     class_ids=[]\n",
    "    confidences=[]\n",
    "    boxes=[]\n",
    "\n",
    "    # pour chaque détection de chaque couche de sortie, obtenez la confiance, l'ID de classe, les paramètres \n",
    "    # de la boîte englobante et ignorez les détections faibles (confiance <0,5) pour chaque détection de chaque \n",
    "    # couche de sortie, obtenir les paramètres de confiance, d'identification de classe, de boîte englobante et \n",
    "    # ignorer les détections faibles (confiance < 0,5)\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence >= 0.5:\n",
    "                #onject detected\n",
    "                center_x= int(detection[0]*width)\n",
    "                center_y= int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                #rectangle co-ordinaters\n",
    "                x=int(center_x - w/2)\n",
    "                y=int(center_y - h/2)\n",
    "                #cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "                boxes.append([x,y,w,h]) # put all rectangle areas\n",
    "                confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "#                 class_ids.append(class_id) # name of the object tha was detected\n",
    "\n",
    "\n",
    "        '''\n",
    "\n",
    "        Il peut y avoir des cas où le même objet peut être détecté plusieurs fois comme ci-dessous. Vous voyez \n",
    "        que deux boîtes sont détectées chacune pour l'ordinateur portable et le moniteur. \n",
    "        Pour éliminer cela, nous utiliserons la fonctionnalité de suppression non maximale (NMS) qui permettar\n",
    "        d'éliminer les boîtes en utilisant une valeur de seuil de 0,6 et permet de conserver que la meilleure \n",
    "        de toutes les boîtes. Et la variable index gardera une trace de ces objets uniques détectés.Donc pas de\n",
    "        détection multiple pour le même objet.\n",
    "\n",
    "        Ensuite, en utilisant la boucle ci-dessous sur toutes les boîtes trouvées, si la boîte apparaît dans \n",
    "        les index, l'algorithme dessinera uniquement un rectangle avec une couleur spécifique, met le texte du \n",
    "        nom de sa classe dessus.\n",
    "\n",
    "        '''\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "#                 indexes = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESH, nms_thresh)\n",
    "\n",
    "\n",
    "\n",
    "    centroid_dict = dict()\n",
    "    FONT = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "#     FONT_SCALE = 1.0\n",
    "    FONT_THICKNESS = 2\n",
    "#     TEST_COLOR = (255, 255, 255)\n",
    "#     label_color = (0, 0, 0)\n",
    "    \n",
    "    for i in indexes.flatten():\n",
    "        x,y,w,h = boxes[i]\n",
    "#         label = '{} {:.2f}%'.format(classes[class_ids[i]],  confidences[i]*100)\n",
    "#         color = colors[class_ids[i]]\n",
    "        color = (255, 255, 0)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color, FONT_THICKNESS)\n",
    "\n",
    "#         (label_width, label_height), baseline = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)\n",
    "#         label_patch = np.zeros((label_height + baseline, label_width, 3), np.uint8)\n",
    "#         cv2.rectangle(frame, (x,y+int(0.5*baseline)),(x+label_width,y-label_height), color, FONT_THICKNESS-3)\n",
    "#         cv2.putText(frame, label, (x,y), FONT, 1, TEST_COLOR, FONT_THICKNESS-1)\n",
    "        centroid_dict[i] = (x+int(w/2.0), y+int(h/2.0), x, y, x+w, y+h)\n",
    "        \n",
    "        \n",
    "    red_zone_list = []\n",
    "    for (id1, p1), (id2, p2) in combinations(centroid_dict.items(), 2):\n",
    "        dx, dy = p1[0] - p2[0], p1[1] - p2[1]\n",
    "        distance = math.sqrt(dx **2 + dy **2)\n",
    "        if distance < 100.0:\n",
    "            if id1 not in red_zone_list:\n",
    "                red_zone_list.append(id1)\n",
    "            if id2 not in red_zone_list:\n",
    "                red_zone_list.append(id2)\n",
    "            \n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    text = \"Social Distancing Violations: {}\".format(len(red_zone_list))\n",
    "    cv2.putText(frame, text, (10, int(0.1*frame.shape[0])), font, 0.85, (0, 0, 255), 2)\n",
    "            \n",
    "    for id, box in centroid_dict.items():\n",
    "            if id in red_zone_list:\n",
    "                cv2.rectangle(frame, (box[2], box[3]), (box[4], box[5]), (0, 0, 255), 2)\n",
    "            else:\n",
    "                continue\n",
    "#                 cv2.rectangle(frame, (box[2], box[3]), (box[4], box[5]), (255, 255, 0), 2)\n",
    "            \n",
    "        \n",
    "    cv2.putText(frame, dt, (700, 710), font, 1, (210, 195, 155), 2, cv2.LINE_8)\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        break\n",
    "    video_outp.write(frame)\n",
    "    \n",
    "    \n",
    "cap.release()\n",
    "video_outp.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
